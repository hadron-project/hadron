<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Hadron | event streaming, workflow orchestration &amp; messaging</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="The Hadron user guide">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        
        <link rel="stylesheet" href="css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><li class="part-title">Overview</li><li class="chapter-item expanded "><a href="index.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="overview/kubernetes.html"><strong aria-hidden="true">2.</strong> Kubernetes</a></li><li class="chapter-item expanded "><a href="overview/events.html"><strong aria-hidden="true">3.</strong> Events</a></li><li class="chapter-item expanded "><a href="overview/streams.html"><strong aria-hidden="true">4.</strong> Streams</a></li><li class="chapter-item expanded "><a href="overview/pipelines.html"><strong aria-hidden="true">5.</strong> Pipelines</a></li><li class="chapter-item expanded "><a href="overview/exchanges.html"><strong aria-hidden="true">6.</strong> Exchanges</a></li><li class="chapter-item expanded "><a href="overview/endpoints.html"><strong aria-hidden="true">7.</strong> Endpoints</a></li><li class="chapter-item expanded "><a href="overview/producers-consumers.html"><strong aria-hidden="true">8.</strong> Producers &amp; Consumers</a></li><li class="chapter-item expanded affix "><li class="part-title">Use Cases</li><li class="chapter-item expanded "><a href="usecases/service-provisioning.html"><strong aria-hidden="true">9.</strong> Service Provisioning</a></li><li class="chapter-item expanded affix "><li class="part-title">Reference</li><li class="chapter-item expanded "><a href="reference/tokens.html"><strong aria-hidden="true">10.</strong> Tokens</a></li><li class="chapter-item expanded "><a href="reference/streams.html"><strong aria-hidden="true">11.</strong> Streams</a></li><li class="chapter-item expanded "><a href="reference/pipelines.html"><strong aria-hidden="true">12.</strong> Pipelines</a></li><li class="chapter-item expanded "><a href="reference/exchanges.html"><strong aria-hidden="true">13.</strong> Exchanges</a></li><li class="chapter-item expanded "><a href="reference/endpoints.html"><strong aria-hidden="true">14.</strong> Endpoints</a></li><li class="chapter-item expanded "><a href="reference/clients.html"><strong aria-hidden="true">15.</strong> Clients</a></li><li class="chapter-item expanded "><a href="reference/cli.html"><strong aria-hidden="true">16.</strong> CLI</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">Hadron | event streaming, workflow orchestration &amp; messaging</h1>

                    <div class="right-buttons">
                        
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#introduction" id="introduction">Introduction</a></h1>
<p>Hadron is a distributed data storage system designed to rapidly ingest large amounts of data, and to facilitate working with that data in the form of arbitrarily complex structured workflows.</p>
<p>Building distributed applications can be tough. Teams might have tens, hundreds or even thousands of microservices. Platforms may have thousands of data signals ranging from business critical application events, to telemetry signals including logs, tracing, metrics and the like. All of this data is important, and now more than ever teams need a way to not only capture this data, but also to work with this data in a scalable and extensible way.</p>
<p>Hadron offers a powerful solution to these problems using the following primitives:</p>
<ul>
<li><strong>Events</strong> - all data going into and coming out of Hadron is structured in the form of events.</li>
<li><strong>Streams</strong> - durable logs for storing arbitrary data, with absolute ordering and horizontal scalability.</li>
<li><strong>Pipelines</strong> - workflow orchestration for data on Streams, providing structured concurrency for arbitrarily complex multi-stage tasks.</li>
<li><strong>Exchanges</strong> - ephemeral messaging hubs used to exchange non-durable events between processes, perfect for GraphQL Subscriptions, WebSockets, Push Notifications and the like.</li>
<li><strong>Endpoints</strong> - general-purpose RPC handlers for leveraging Hadron's powerful networking capabilities.</li>
<li><strong>Producers</strong> - client processes connected to Hadron, written in any language, working to produce and publish data to Hadron.</li>
<li><strong>Consumers</strong> - client processes connected to Hadron, written in any language, working to consume data from Streams, process Pipeline stages, consume ephemeral messages from Exchanges, or even handle RPC Endpoints.</li>
</ul>
<p>Hadron was born into the world of Kubernetes, and Kubernetes is a core assumption in the Hadron operational model. See the next section for more details.</p>
<p>For a use case demonstrating the power of these components working together, head on over to <a href="./usecases/service-provisioning.html">Use Case: Service Provisioning</a>.</p>
<h1><a class="header" href="#hadron--kubernetes" id="hadron--kubernetes">Hadron &amp; Kubernetes</a></h1>
<blockquote>
<p>Kubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications.
<br/><small><i>~ <a href="https://kubernetes.io/">kubernetes.io</a></i></small></p>
</blockquote>
<p>Kubernetes has become a cornerstone of the modern cloud ecosystem, and Hadron is purpose built for the Kubernetes platform. Hadron is native to Kubernetes. It was born here and it knows the ins and outs.</p>
<p>Hadron is designed from the ground up to take full advantage of Kubernetes and its rich API for deploying and running applications. Building upon this foundation has enabled Hadron to achieve an operational model with a simple setup, which removes performance bottlenecks, simplifies clustering and consensus, provides predictable and clear scalability, and positions Hadron for seamless and intuitive integration with user applications and infrastructure.</p>
<p>Each of the above points merits deeper discussion, all of which are covered in further detail throughout the reference section of this guide. Here are the highlights.</p>
<h3><a class="header" href="#simple-setup" id="simple-setup">Simple Setup</a></h3>
<p>Most usage of Hadron will start with the Hadron Helm chart. The chart installs the Hadron Operator along with various other resources. Everything related to Hadron operations is handled by the Operator and is driven via Kubernetes configuration files. Provisioning, auto-scaling, networking, access control, all of this is controlled through a few lines of YAML which can be versioned in source control and reviewed as code.</p>
<p>The entire lifecycle of Hadron clusters is handled by the Operator. Upgrading a cluster to a new version of Hadron, horizontally scaling a cluster, adding credentials and access control, all of this and more is declarative and fully managed, which means no operational headaches for users.</p>
<h3><a class="header" href="#removing-performance-bottlenecks" id="removing-performance-bottlenecks">Removing Performance Bottlenecks</a></h3>
<p>Kubernetes offers deployment models which allow Hadron to completely remove the need for its own distributed consensus algorithms in many cases, especially on the write path of publishing data to streams. Without this overhead, Hadron is able to optimize write throughput, replication, and other performance criteria in ways which would not otherwise be feasible.</p>
<p>These wins come with no downsides. Horizontally scaling a cluster is still fully dynamic, clients are still able to detect cluster topology changes in real-time as they take place, but most importantly: the path which data takes from the moment of publication to the moment it is persisted to disk is direct and simple. No extra network hops. No overhead. Just an HTTP2 data stream directly from the client to the Hadron internal function which writes data to disk. All with absolute ordering per partition.</p>
<h3><a class="header" href="#seamless-and-intuitive-integration" id="seamless-and-intuitive-integration">Seamless and Intuitive Integration</a></h3>
<p>Hadron clusters are exposed for application and infrastructure integration using canonical Kubernetes patterns for networking and access. Clients receive a stream of cluster metadata and can react in real-time to topology changes to maximize use of horizontal scaling and high-availability.</p>
<p>If your applications are already running in Kubernetes, then integration couldn't be more simple.</p>
<h1><a class="header" href="#events" id="events">Events</a></h1>
<blockquote>
<p>A specification for describing event data in a common way
<br/><small><i>~ <a href="https://cloudevents.io/">cloudevents.io</a></i></small></p>
</blockquote>
<p>Events are everywhere. Everything is an event. As humans have continued to work with event-driven data streams, we've accumulated many best practices on how to model our data in a reliable, extensible, and intuitive way. CloudEvents is at the heart of this movement, and also happens to be at the very core of Hadron.</p>
<p>Everything in Hadron is an event, a CloudEvents 1.0 event. Streams, Pipeline inputs and outputs for workflow orchestration, ephemeral messages, everything in Hadron is built around the CloudEvents model.</p>
<h3><a class="header" href="#ease-of-use" id="ease-of-use">Ease of Use</a></h3>
<p>Hadron events are well-structured, easy to analyze, and prime for automation and application usage.</p>
<h3><a class="header" href="#interoperable" id="interoperable">Interoperable</a></h3>
<p>Hadron is positioned from the very beginning to integrate seamlessly with the greater event-driven ecosystem.</p>
<h3><a class="header" href="#ready-to-go" id="ready-to-go">Ready to Go</a></h3>
<p>Hadron clients speak CloudEvents fluently. Publishing data to, and consuming data from Hadron is clear, concise, and has broad general-purpose application.</p>
<h1><a class="header" href="#streams" id="streams">Streams</a></h1>
<p>Streams are append-only, immutable logs of data with absolute ordering per partition.</p>
<p>Streams are deployed as independent StatefulSets within a Kubernetes cluster, based purely on a Stream CRD (YAML config) stored in Kubernetes. Each pod of the StatefulSet acts as the leader of a partition of its Stream. Kubernetes guarantees the identity, stable storage and stable network address of each pod. When replication is enabled for a Stream, replication is based on a deterministic algorithm and does not require leadership or consensus given these powerful identity properties from the StatefulSet building block.</p>
<h3><a class="header" href="#scaling" id="scaling">Scaling</a></h3>
<p>Streams can be horizontally scaled based on the Stream's CRD. Scaling a Stream to add new partitions will cause the Hadron Operator to scale the corresponding StatefulSet, which ultimately adds a matching number of pods.</p>
<p>Each pod constitutes an independent container process with its own set of system resources (CPU, RAM, storage), all of which will span different underlying virtual or hardware nodes (OS hoststs) of the underlying Kubernetes cluster. This helps to ensure availability of the Stream.</p>
<h3><a class="header" href="#high-availability-ha" id="high-availability-ha">High Availability (HA)</a></h3>
<p>In Hadron, the availability of a Stream is reckoned as a whole. If any partition of the Stream is alive and running, then the Stream is available. The temporary loss of an individual partition does not render the Stream overall as being unavailable.</p>
<p>This is a powerful property of Hadron, and is an intentional design decision which the Hadron ecosystem builds upon. Hadron clients automatically monitor the topology of the connected Hadron cluster, and will establish connections to new partitions as they become available, and will failover to healthy partitions when others become unhealthy. Hashing to specific partitions based on the <code>subject</code> of an event or event batch is still the norm, however clients also dynamically take into account the state of connections as part of that procedure.</p>
<h3><a class="header" href="#producers" id="producers">Producers</a></h3>
<p>Hadron clients which produce and publish data to a Hadron Stream (or other component) are considered to be Producers. Stream Producers establish durable long-lived connections to all partitions of a Stream, and will publish data to specific partitions based on hashing the <code>subject</code> of an event or event batch.</p>
<h3><a class="header" href="#consumers" id="consumers">Consumers</a></h3>
<p>Hadron clients which consume data from a Stream (or other component) are considered to be Consumers. Stream Consumers establish durable long-lived connections to all partitions of a Stream, and consume data from all partitions. Every event consumed includes details on the ID of the event as well as the partition from which it came. This combination establishes uniqueness, and also happens to be a core facet of the CloudEvents ecosystem.</p>
<p>Consuming data from a Stream is transactional in nature, and the lifetime of any transaction is tied to the client's connection to the corresponding Stream partition. If a Consumer's connection is ever lost, then any outstanding deliveries to that Consumer are considered to have failed and will ultimately be delivered to another Consumer for processing.</p>
<h4><a class="header" href="#groups" id="groups">Groups</a></h4>
<p>Stream Consumers may form groups. Groups are used to load-balance work across multiple processes working together as a logical Consumer.</p>
<p>All consumers must declare a group when they first establish a connection to the backing Stream partitions. When multiple consumers claim to be participating in a group bearing the same name, then they are implicitly members of the same group.</p>
<h4><a class="header" href="#durable-or-ephemeral" id="durable-or-ephemeral">Durable or Ephemeral</a></h4>
<p>Stream Consumers may be durable or ephemeral. Durable groups will have their progress recorded on each respective Stream partition. Ephemeral groups only have their progress tracked in memory, which is erased once all group members disconnect per partition.</p>
<h1><a class="header" href="#pipelines" id="pipelines">Pipelines</a></h1>
<p>Pipelines are workflow definitions for data on Streams, providing structured concurrency for arbitrarily complex multi-stage tasks.</p>
<p>Pipelines exist side by side with their source Stream, and Streams may have any number of associated Pipelines. Pipelines are triggered for execution when an event published to a Stream has an event <code>type</code> which matches one of the trigger patterns of an associated Pipeline.</p>
<h3><a class="header" href="#why" id="why">Why</a></h3>
<p>So, why do Pipelines exist, and what are they for?</p>
<p>Practically speaking, as software systems grow, they will inevitibly require sequences of tasks to be executed, usually in some logical ordering, and often times these tasks will cross system/service/microservice boundaries. When a system is young, such workflows might be simple, and may involve only one or two stages — such as making an API call and then emailing a user. However, as the system evolves, these workflows will grow in the number of stages, and ordering may become difficult.</p>
<p>When companies are young, these asynchronous tasks may be implicit. They exist in code, but their actual role in the system overall may be easily forgotten, and tracking down individual functions for these tasks may not be easy. The workflow may not even have a name.</p>
<p>Pipelines offer a way to name these workflows, to define them as declarative specifications so that they can be versioned and reviewed. Pipelines are a way to avoid sprawl, confusion, and to bring clarity to how a software system actually operates at the highest levels.</p>
<p>Pipelines can be used to define the entire logical composition of a company's software systems. No more unnamed workflows which exist only as tribal knowledge in the minds of a few developers. No more grueling debug sessions where the first few hours are spent just trying to trace an implicit, unnamed, multi-stage workflow across multiple microservices.</p>
<h3><a class="header" href="#scaling--high-availability" id="scaling--high-availability">Scaling &amp; High Availability</a></h3>
<p>Pipelines exist side by side with their source Stream. All scaling and availability properties of the source Stream apply to any and all Pipelines associated with that Stream. See the <a href="overview/./streams.html">Streams Overview</a> for more details on these properties.</p>
<h3><a class="header" href="#publishers" id="publishers">Publishers</a></h3>
<p>Pipelines do not have their own direct mechanism for publishing data to a Pipeline. Instead, data is published to the Pipeline's source Stream, and when an event on that source Stream has a <code>type</code> field which matches one of the Pipeline's trigger patterns, then a new Pipeline execution will be started with that event as the &quot;root event&quot; of the Pipeline execution.</p>
<h3><a class="header" href="#triggers" id="triggers">Triggers</a></h3>
<p>Every Pipeline may be declared with zero or more <code>triggers</code>. When an event is published to a Pipeline's source Stream, its <code>type</code> field will be compared to each of the matcher patterns in the Pipeline's <code>triggers</code> list. If any match is found, then a new Pipeline execution will begin for that event.</p>
<p>If a Pipeline is declared without any <code>triggers</code>, or with a trigger which is an empty string (<code>&quot;&quot;</code>), then it will match every event published to its source Stream.</p>
<h3><a class="header" href="#consumers-1" id="consumers-1">Consumers</a></h3>
<p>Pipelines are consumed in terms of their stages. As Hadron client programs register as Pipeline consumers, they are required to speciy the stage of the Pipeline which they intend to process. All Pipeline consumers form an implicit group per stage.</p>
<h3><a class="header" href="#pipeline-evolution" id="pipeline-evolution">Pipeline Evolution</a></h3>
<p>As software systems evolve over time, it is inevitible that Pipelines will also evolve. Pipelines may be safely updated in many different ways, the only dangerous update is to remove a Pipeline's stage. Doing so should ALWAYS be considered to result in data loss. These semantics may change in the future, however it is best avoided.</p>
<p>Barring removal of Pipeline stages, most other updates are perfectly safe and encouraged. Adding new stages, changing dependencies and ordering, all of this is perfectly fine and expected.</p>
<p>There is no renaming of Pipeline stages, this is tantamount to deleting a stage and adding a new stage with a different name.</p>
<h4><a class="header" href="#updating-consumers" id="updating-consumers">Updating Consumers</a></h4>
<p>As Pipelines evolve, users should take care to ensure that their applications have been updated to process any new stages added to the Pipeline. Hadron makes this very simple:</p>
<ul>
<li>Before applying the changes to the Pipeline which adds new stages, first update the application's Pipeline consumers.</li>
<li>Add a consumer for any new stages.</li>
<li>Deploy the new application code. The new consumers will log errors as they attempt to connect, as Hadron will reject the consumer registry until the new stages are applied to the Pipeline. This is perfectly fine, and will not crash the application code. Hadron will simply back off, and retry the connection again soon.</li>
<li>Now it is safe to apply the changes to the Pipeline.</li>
</ul>
<p>In essence: add your Pipeline stage consumers first.</p>
<p>If this protocol is not adhered to, then the only danger is that the Pipeline will eventually stop making progress, as too many parallel executions will remain in an uncomplete state as they wait for the new Pipeline stages to be processed. Avoid this by deploying your updated application code first.</p>
<h1><a class="header" href="#exchanges" id="exchanges">Exchanges</a></h1>
<p>These docs are currently under construction.</p>
<ul>
<li>intended for ephemeral events,</li>
<li>events are dropped after being consumed or are dropped immediately if there are no consumers,</li>
<li>perfect for ephemeral event streams such as GraphQL Subscriptions, WebSockets, Push Notifications and the like,</li>
</ul>
<h1><a class="header" href="#rpc-endpoints" id="rpc-endpoints">RPC Endpoints</a></h1>
<p>These docs are currently under construction.</p>
<ul>
<li>an abstraction built directly on top of gRPC,</li>
<li>can be thought of as something similar to brokered gRPC,</li>
<li>clients are configured with automatic serialization &amp; deserialization for nearly identical look and feel of plain old gRPC,</li>
</ul>
<h1><a class="header" href="#producers--consumers" id="producers--consumers">Producers &amp; Consumers</a></h1>
<p>Hadron clients which produce and publish data to a Stream, an Exchange or an Endpoint are considered to be Producers. Hadron clients which consume data from a Stream, an Exchange or an Endpoint are considered to be Consumers.</p>
<p>Producers and Consumers establish durable long-lived connections to backend components in the target Hadron cluster, which helps to avoid unnecessary setup and teardown of network connections.</p>
<p>Producers and Consumers typically exist as user defined code within larger applications. However, they may also exist as open source projects which run independently based on runtime configuration, acting as standalone components, often times both producing and consuming data. The latter are typically referred to as Connectors.</p>
<p>Producers and Consumers may be created in any language. The Hadron team maintains a common Rust client which is used as the shared foundation for clients written in other languages, which provides maximum performance and safety across the ecosystem. The Hadron team also maintains the Hadron CLI, which is based upon the Rust client and which can be used for basic production and consumption of data from Hadron.</p>
<p>Producer and Consumer behavior varies depending on where data is being published to or consumed from, whether for Streams, Exchanges or for RPC Endpoints. Each respective section of the guide contains further details on these differences.</p>
<h1><a class="header" href="#use-case-service-provisioning" id="use-case-service-provisioning">Use Case: Service Provisioning</a></h1>
<h3><a class="header" href="#getting-started" id="getting-started">Getting Started</a></h3>
<p>We've joined a new company, ExampleCloud, which offers a service where customers may provision various types of storage systems in the cloud. Though conceptually simple, there are lots of individual stages in this workflow, each of which will take different amounts of time to complete, and all having different failure conditions and data requirements.</p>
<p>With Hadron, defining a Pipeline to model this workflow is simple:</p>
<pre><code class="language-yaml">---
apiVersion: hadron.rs/v1beta1
kind: Stream
metadata:
  name: events
spec:
  partitions: 3

---
apiVersion: hadron.rs/v1beta1
kind: Pipeline
metadata:
  name: service-creation
spec:
  sourceStream: events
  triggers:
    - service.created
  stages:
    - name: deploy-service
</code></pre>
<p>Here we've define a Stream called <code>events</code> and a Pipeline called <code>service-creation</code> to model this workflow. Hadron uses this config to generate resources within its cluster to store and process the data for this new Stream and Pipeline.</p>
<p>With this configuration, any time our ExampleCloud application publishes a new event of type <code>service.created</code> to our Stream <code>events</code>, Hadron will automatically trigger a new Pipeline execution which will pass that new event through the Pipeline stages defined above (right now, only 1 stage).</p>
<h3><a class="header" href="#client-setup" id="client-setup">Client Setup</a></h3>
<p>To get started, we will create a program which uses the Hadron Client to publish events of type <code>service.created</code> to our Stream <code>events</code>, and then we will also create a subscription to our new Pipeline which will process the stage <code>deploy-service</code>.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Event producer which publishes events to stream `events`.
let client = hadron::Client::new(&quot;cluster.url:7000&quot;, /* ... */)?;
let publisher = client.publisher(&quot;example-cloud-app&quot;).await?;

// Publish a new event based on application logic.
publisher.publish(new_event).await?;
<span class="boring">}
</span></code></pre></pre>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Process Pipeline stage `deploy-service`.
client.pipeline(&quot;service-creation&quot;, &quot;deploy-service&quot;, deploy_handler);
<span class="boring">}
</span></code></pre></pre>
<p>Awesome! The consumer code (the function <code>deploy_handler</code> above) can do whatever it wants. The only requirement is that when it is done, it must return a <code>Result&lt;NewEvent, Error&gt;</code> — that is, it must return an output event for success, or an error for failure cases (resulting in a retry).</p>
<p>Client disconnects will trigger retries. Errors will be tracked by Hadron and exposed for monitoring. Output events from successful processing of stages are persisted to disk. Once a stage is completed successfully for an event, that stage will never be executed again for the same event.</p>
<h3><a class="header" href="#new-requirements" id="new-requirements">New Requirements</a></h3>
<p>Things are going well at our new company. Service creation is trucking along, deploying services for customers, and life is good. However, as it turns out, there are a few important steps which we've neglected, and our boss would like to have that fixed.</p>
<p>First, we forgot to actually charge the customer for their new services. Company isn't going to survive long unless we start charging, so we'll need to add a new stage to our Pipeline to handle that logic.</p>
<p>Next, customers have expressed that they would really like to know the state of their service, so once their service has been deployed, we'll need to deploy some monitoring for it. Let's add a new stage for that as well.</p>
<p>Finally, customers are also saying that it would be great to receive a notification once their service is ready. Same thing, new stage.</p>
<p>With all of that, we'll update the Pipeline's stages section to look like this:</p>
<pre><code class="language-yaml">  stages:
    - name: deploy-service
    - name: setup-billing
      dependencies: [&quot;deploy-service&quot;]
    - name: setup-monitoring
      dependencies: [&quot;deploy-service&quot;]
    - name: notify-user
      dependencies: [&quot;deploy-service&quot;]
</code></pre>
<p>Pretty simple, but let's break this down. First, the original <code>deploy-service</code> stage is still there and unchanged. Next, we've added our 3 new stages <code>setup-billing</code>, <code>setup-monitoring</code>, and <code>notify-user</code>. There are a few important things to note here:</p>
<ul>
<li>Each of the new stages depends upon <code>deploy-service</code>. This means that they will not be executed until <code>deploy-service</code> has completed successfully and produced an output event.</li>
<li>Once <code>deploy-service</code> has completed successfully, our next 3 stages will be executed in parallel. Each stage will receive a copy of the root event which triggered this Pipeline execution, as well as a copy of the output event of any stages declared as a dependency.</li>
</ul>
<p>This compositional property of Hadron Pipelines sets it apart from the crowd as a powerful data processing and data integration system.</p>
<p>Given that we've added new stages to our Pipeline, we need to add some new stage consumers to actually handle this logic. This is nearly identical to our consumer for <code>deploy-service</code>, really only the business logic in the handler will be different for each.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>client.pipeline(&quot;service-creation&quot;, &quot;setup-billing&quot;, billing_handler).await?;
client.pipeline(&quot;service-creation&quot;, &quot;setup-monitoring&quot;, monitoring_handler).await?;
client.pipeline(&quot;service-creation&quot;, &quot;notify-user&quot;, notify_handler).await?;
<span class="boring">}
</span></code></pre></pre>
<h3><a class="header" href="#analytics" id="analytics">Analytics</a></h3>
<p>The team is quite happy with the ease of extending our Pipeline and adding new stage consumers. However, the company has continued to grow, and we've agreed that some deeper analytics would be great as a final touch.</p>
<p>In this case, we would like to evaluate execution time, errors, re-delivery rates, and other such metadata on our stage handlers. Let's add a new stage to our Pipeline.</p>
<pre><code class="language-yaml">    - name: analytics
      dependencies:
        - deploy-service
        - setup-billing
        - setup-monitoring
        - notify-user
</code></pre>
<p>The <code>analytics</code> stage we've defined here will integrate the data signal from all previous stages in the Pipeline, receiving a copy of the root event and output events of each stage. With all of this data, we can generate metrics based on event metadata or even application specific data within each event.</p>
<p>The sky is the limit. If needed, we could transactionally materialize this data into a traditional RDBMS like PostgreSQL, or we could ship this data over to other systems for graphing, metrics, alerting and the like. As we've seen, adding new stages to a Pipeline is straightforward. Backfilling stages to take place earlier in a Pipeline's workflow is possible, and even adding that output as a dependency to other existing stages is fully backwards compatible.</p>
<h1><a class="header" href="#tokens" id="tokens">Tokens</a></h1>
<p>These docs are currently under construction.</p>
<!--
Hadron uses a simple but effective permissions model. There are users and there are tokens. Users represent administrators of the Hadron cluster, while tokens represent access grants to specific Hadron resources for use in application code and automation tools.

There are a few different user roles:
- `Owner`: this user has full control over the cluster, all namespaces and all resources.
- `Admin`: admins are the same as owners, except that admin permissions may be revoked by other admins and owners, but admins can not revoked the permissions of an owner.
- `Viewer`: viewers are read-only. They may view any of ther resources of the cluster, but may not modify them.

Users are not allowed to directly use the resources of Hadron, that is what tokens are for. Tokens represent a set of permissions for the bearer of that token. The token ID is retained within Hadron and the token may be deleted, which revokes that token's access to the cluster. Tokens come in a few forms:
- `All`: a permissions grant on all resources in the system.
- `Namespaced`: a set of permissions granted on namespace scoped resources.
- `Metrics`: A permissions grant on only the cluster metrics system.

Namespace grants come in two different forms:
- `Full`: a grant of full permissons on the target namespace.
    - `namespace`: the namespace to which the grant pertains.
- `Limited`: a grant of limited access to specific resources within the target namespace.
    - `namespace`: the namespace to which the grant pertains.
    - `messaging`: an optional `pub/sub/all` enum value indicating access to the namespace's ephemeral messaging exchange.
    - `endpoints`: a list of endpoint permissions with the following structure:
        - `matcher`: the endpoint name matcher to use. May include a wildcard to match endpoint hierarchies. Same wildcard rules apply as described in the [ephemeral messaging chapter](./ephemeral-messaging.md).
        - `access`: a `pub/sub/all` enum value.
    - `streams`: a list of stream permissions with the following structure:
        - `matcher`: the stream name matcher to use. May include a wildcard to match streams hierarchically. Same wildcard rules apply as described in the ephemeral messaging chapter.
        - `access`: a `pub/sub/all` enum value.
    - `schema`: a boolean indicating if the token has permissions to modify the schema of the namespace.

In addition to the above:
- Hadron clusters are initialized with a default `root:root` user bearing the `Owner` role. It is expected that cluster admins will use the root credentials to initialize any other users for the system, and it is expected that the root password will be changed and stored securely or removed in favor of other credentials.
- User & token management is performed via the Hadron CLI. -->
<h1><a class="header" href="#streams-1" id="streams-1">Streams</a></h1>
<p>These docs are currently under construction.</p>
<!--
Streams are append-only, immutable logs of data.

## Schema
Streams are declared in YAML as part of the [Schema Management system](./schema.md). The schema for the `Stream` object is as follows:

```yaml
## The kind of object being defined. In this case, a stream.
kind: Stream
## The namespace in which this stream is to be created.
namespace: required string
## The name of the stream. Each stream must have a unique name per namespace.
name: required string

## The cluster replica sets which are to be used as partitions for this stream.
partitions: required string

## An optional TTL duration specifying how long records are to be kept on
## the stream.
##
## If not specified, then records will stay on the stream forever.
ttl: optional duration (default none)
```

Streams can be updated the same way all other Hadron DDL objects can be updated. See the [Schema Management chapter](./schema.md) for more details.

### Details
- Stream names may be 1-100 characters long, containing only `[-_.a-zA-Z0-9]`. The `.` can be used to form hierarchies for authorization matching wildcards.
- Streams may have one or more partitions, which may be increased as needed. Horizontally scaling the Hadron cluster and adding more partitions to a stream will allow the write throughput of the stream to horizontally scale.
- Stream data is replicated across each partition's replica set.
- Streams must first be created via the Hadron schema system. See the [Schema Management](./schema.md) chapter for more details.

## Subscriptions
Subscriptions represent a client's interest to consume data from a stream. Subscriptions are created when a client submits a subscription request. Subscription requests must specify the target stream to subscribe to and a name to use for the subscription.

**Consumers & groups:** clients which process records from a stream as part of a subscription are known as consumers. When multiple clients subscribe to the same stream using the same subscription name, this will dynamically form a consumer group and messages will be load-balanced across all members of the group. Consumers will receive all messages without any message level discrimination.

**Durable:** subscriptions are durable in the sense that the group's progress through the stream is stored on disk. Clients may disconnect at any time, and they will be automatically removed from the consumer group, and new consumers may be added to the group just as easily.

**Starting point:** when subscriptions are created, they may specify a starting point: first or latest. The starting point only affects the creation of the consumer group when a subscription is created.

**Subscription Config:** when creating a subscription, there are various configuration options which my be specified in order to control behavior.
- `maxParallelConsumers` (default none): the maximum number of consumers allowed to process messages in parallel. If set to 1, then only one consumer is allowed to process messages at a time for the consumer group.

**Consumer Config:** each individual consumer within a group has its own isolated configuration options.
- `batchSize` (default 100): the maximum number of messages which will be delivered to the consumer per batch.
- `batchWaitMillis` (default 500): the amount of time in milliseconds which the consumer should delay for its batch to fill. This only applies when the consumer's batch has been partially filled.

Subscriptions may be deleted using the `StreamUnsub` client API.

### Ack & Nack
Messages being consumed from a stream must be ack'ed. Once a message is ack'd, it will not be delivered again to the same consumer group.

Messages may be nack'ed, which will cause immediate redelivery by default. A redelivery timeout may be specified, which will cause a timeout to be applied before redelivery of the message to a consumer. Redelivery timeouts are not durable, and are held in-memory by Hadron.

If a client disconnects while it was processing unacknowledged messages, Hadron will redeliver those messages to other live consumers of the same subscription consumer group. -->
<h1><a class="header" href="#pipelines-1" id="pipelines-1">Pipelines</a></h1>
<p>These docs are currently under construction.</p>
<!--
Pipelines are multi-stage data workflows, composed of multiple streams, structured as a directed acyclic graph (DAG). Pipelines provide transactional guarantees for multi-stage asynchronous workflows. Pipelines orchestrate the delivery of events to specific pipeline stages, collect outputs from pipeline stages, and enforce stage execution order. Pipelines provide a source of truth for codifying asynchronous event-driven architectures.

## Schema
Pipelines are declared in YAML as part of the [Schema Management system](./schema.md). The schema for the `Pipeline` object is as follows:

```yaml
## The kind of object being defined. In this case, a pipeline.
kind: Pipeline
## The namespace in which this pipeline is to be created.
namespace: required string
## The name of the pipeline. Each pipeline must have a unique name per namespace.
name: required string

## The stream from which this pipeline may be triggered. Only events on this
## stream will trigger this pipeline. The trigger stream must exist in the
## same namespace as the pipeline.
inputStream: required string

## Optional event types of events on the input stream which should trigger new
## instances of this pipeline.
##
## If no triggers are decalred, then all events published to the input stream
## will create new pipeline instances.
triggers:
  - string

## `optional array of <stage>`: An array all stages of which this pipeline is
## composed. Pipelines are composed of one or more stages.
stages:
  ## The name of the stage. Each stage has a unique name per pipeline.
  - name: required string

    ## `optional array of string`: A stage will be executed after all of the
    ## stages in its `after` array have successfully completed.
    ##
    ## Execution order must be acyclic. If this value is empty or omitted,
    ## then this stage is considered to be an initial stage of the pipeline,
    ## and will be invoked with a copy of the event which triggered
    ## the pipeline.
    after:
      ## The name of another stage in this pipeline.
      - required string

    ## `optional array of string`: Stages may depend upon the output of
    ## earlier stages, which also implies an `after` relationship with the
    ## specified stages.
    ##
    ## If no dependencies are declared, then the stage will be provided with
    ## the root event which triggered the respective pipeline instance. The
    ## root event can be referred to directly as `root_event` along with
    ## other stages.
    dependencies:
      ## The name of a stage from this pipeline, or the special name
      ## `root_event` which refers to the root event of a pipeline instance.
      - required string
```

### Details
- Pipelines can be updated the same way all other Hadron schema objects can be updated. See the [Schema Management](./schema.md) chapter for more details.
- Pipeline names may be 1-100 characters long, containing only `[-_.a-zA-Z0-9]`. The `.` can be used to form hierarchies for authorization matching wildcards.
- Pipelines may be composed of 1 or more stages (nodes of the graph).
- The flow of data through the pipeline — inputs and outputs — are the edges of the graph.
- Pipelines may have more than one initial stage. All initial stages of a pipeline will receive a copy of the event which triggered the pipeline.
- Pipelines are exclusive. They represent a single type of action to be taken over the data moving through the pipeline. All consumers of pipeline stages will be treated as being part of the same consumer group and messages will be load balanced across all consumers.
- Pipelines have unique names.
- Pipelines are directed acyclic graphs, cycles are not allowed.
- Hadron ensures that the output of a stage is delivered to all downstream stages with a connected edge, and that the message is `ack`'ed transactionally along with the delivery of that output. This greatly reduces the overhead of error handling and minimizes the difficulties of modelling idempotent workflows.
- Pipeline stages may execute in parallel when stages are children of the same parent stage in the graph, or in serial order when stages are ordered one after another in the graph. The `dependencies` & `after` keywords control this behavior.

## Consumers
Pipeline stages must each be individually subscribed to useing the pipelines subscription API. The pipelines subscription API is uniform for all stages, and allows consumers to think purely in terms of reciving inputs and producing outputs. Hadron consumers are channel based, and multiple channels may exist per network connection to the Hadron cluster. -->
<h1><a class="header" href="#exchanges-1" id="exchanges-1">Exchanges</a></h1>
<p>These docs are currently under construction.</p>
<!--

A topic-based, at most once delivery messaging system, perfect for ephemeral data.

- Provides **at most once delivery semantics.**
- Messages are published with "topics", similar to AMQP-style topics. Defaults to an empty string. No ack or nack is used for ephemeral messages.
- Consumers may specify a "topic" matcher, which expresses interest in matching messages. Wildcard topic matchers are supported, similar to AMQP-style wildcards.
- If no consumer matches the topic of the message, it will be dropped.
- Consumers may form groups, where messages will be load balanced across healthy group members.
- Consumer group information is synchronously replicated to all nodes when the consumer group is formed and as members join and leave the group, but this information is only held in memory.
- Consumer group load balancing decisions are made by the node which received the message needing to be load balanced.
- Messages will be delivered once to each consumer group matching the message's topic.
- Ephemeral messaging exchanges are implicity created as part of a namespace. Namespaces have one and only one ephemeral messaging exchange.

### Topics
Hadron enforces that message topics adhere to the following pattern `[-_A-Za-z0-9.]*`. In English, this could be read as "all alpha-numeric characters, hyphen, underscore and period". Topics are case-sensitive and can not contain whitespace.

##### Topic Hierarchies
The `.` character is used to create a subject hierarchy. A volcanology team might define the following hierarchy for collecting sensor readings on volcanoes they are monitoring, where volcanoes are grouped by the region they are in, followed by the name of the volcano, then followed by the cardinal point where the sensor is stationed with respect to the center of the volcano.

```
volcanoes.usa
volcanoes.usa.atka
volcanoes.usa.kahoolawe.north
volcanoes.tanzania
volcanoes.tanzania.meru
volcanoes.tanzania.kilimanjaro.east
volcanoes.tanzania.kilimanjaro.west
```

### Wildcard Matchers
There are two wildcard tokens available to subscribers for matching message topics. Subscribers can use these wildcards to listen to multiple topics with a single subscription but Publishers will always use a fully specified subject, without any wildcards (as the wildcard characters are not valid topic characters while publishing).

##### Single-Token Matching
The first wildcard is `*` which will match a single hierarchy token. If the volcanology team needs to build a consumer for monitoring everything from `Kilimanjaro`, it could subscribe to `volcanoes.tanzania.kilimanjaro.*`, which would match `volcanoes.tanzania.kilimanjaro.east` and `volcanoes.tanzania.kilimanjaro.west`.

##### Multi-Token matching
The second wildcard is `>` which will match one or more hierachy tokens, and can only appear at the end of the topic. For example, `volcanoes.usa.>` will match `volcanoes.usa.atka` and `volcanoes.usa.kahoolawe.north`, while `volcanoes.usa.*` would only match `volcanoes.usa.atka` since it can’t match more than one hierarchy token.

### Consumers
Ephemeral message consumers specify a topic matcher and may optionally specify a queue group to begin consuming messages. Every consumer which is part of the same queue group will have messages load balanced across the group. -->
<h1><a class="header" href="#rpc-endpoints-1" id="rpc-endpoints-1">RPC Endpoints</a></h1>
<p>These docs are currently under construction.</p>
<!--
RPC Endpoints provide a service-oriented request/response system, akin to traditional REST APIs or other RPC systems like gRPC.

## Schema
RPC Endpoints are declared in YAML as part of the [Schema Management system](./schema.md). The schema for the `Endpoint` object is as follows:

```yaml
## The kind of object being defined. In this case, a pipeline.
kind: Endpoint
## The namespace in which this endpoint is to be created.
namespace: required string
## The name of the endpoint. Each endpoint must have a unique name per namespace.
name: required string
## The input RPC mode.
input: enum Single | Stream
## The output RPC mode.
output: enum Single | Stream
```

### Details
- Endpoint names may be 1-100 characters long, containing only `[-_.a-zA-Z0-9]`. The `.` can be used to form hierarchies for authorization matching wildcards. Consumers do not use wildcards for endpoints.
- RPC messages are published to a specific endpoint within a namespace.
- RPC endpoints are declared explicity in code and must be created on the server.
- RPC messages are not durable, but if an endpoint has no live consumers when a message is published, an error response will be immediately returned for better control flow.

## Consumers
RPC Endpoints offer consumer patterns similar to the ephemeral messaging system, except that wildcards are not allowed. -->
<h1><a class="header" href="#clients" id="clients">Clients</a></h1>
<p>These docs are currently under construction.</p>
<h1><a class="header" href="#cli" id="cli">CLI</a></h1>
<p>Hadron ships with a native CLI (Command-Line Interface), called <code>hadron</code>.</p>
<pre><code>The Hadron CLI

USAGE:
    hadron [FLAGS] [OPTIONS] &lt;SUBCOMMAND&gt;

FLAGS:
    -h, --help       Prints help information
    -V, --version    Prints version information
    -v               Enable debug logging

OPTIONS:
        --token &lt;token&gt;    Set the auth token to use for interacting with the cluster
        --url &lt;url&gt;        Set the URL of the cluster to interact with

SUBCOMMANDS:
    help        Prints this message or the help of the given subcommand(s)
    pipeline    Hadron pipeline interaction
    stream      Hadron stream interaction
</code></pre>
<h3><a class="header" href="#hadron-stream" id="hadron-stream">hadron stream</a></h3>
<pre><code>Hadron stream interaction

USAGE:
    hadron stream &lt;SUBCOMMAND&gt;

FLAGS:
    -h, --help       Prints help information
    -V, --version    Prints version information

SUBCOMMANDS:
    help    Prints this message or the help of the given subcommand(s)
    pub     Publish data to a stream
    sub     Subscribe to data on a stream
</code></pre>
<h4><a class="header" href="#hadron-stream-pub" id="hadron-stream-pub">hadron stream pub</a></h4>
<pre><code>Publish data to a stream

USAGE:
    hadron stream pub [FLAGS] [OPTIONS] &lt;data&gt; --subject &lt;subject&gt; --type &lt;type&gt;

FLAGS:
    -b, --binary     If true, treat the data payload as a base64 encoded binary blob
    -h, --help       Prints help information
    -V, --version    Prints version information

OPTIONS:
    -o &lt;optattrs&gt;...           Optional attributes to associate with the given payload
    -s, --subject &lt;subject&gt;    The subject of the new event
    -t, --type &lt;type&gt;          The type of the new event

ARGS:
    &lt;data&gt;    The data payload to be published
</code></pre>
<h4><a class="header" href="#hadron-stream-sub" id="hadron-stream-sub">hadron stream sub</a></h4>
<pre><code>Subscribe to data on a stream

USAGE:
    hadron stream sub [FLAGS] [OPTIONS] --group &lt;group&gt;

FLAGS:
    -d, --durable            Make the new subscription durable
    -h, --help               Prints help information
        --start-beginning    Start from the first offset of the stream, defaults to latest
        --start-latest       Start from the latest offset of the stream, default
    -V, --version            Prints version information

OPTIONS:
    -b, --batch-size &lt;batch-size&gt;        The batch size to use for this subscription [default: 1]
    -g, --group &lt;group&gt;                  The subscription group to use
        --start-offset &lt;start-offset&gt;    Start from the given offset, defaults to latest
</code></pre>
<h3><a class="header" href="#hadron-pipeline" id="hadron-pipeline">hadron pipeline</a></h3>
<pre><code>Hadron pipeline interaction

USAGE:
    hadron pipeline &lt;SUBCOMMAND&gt;

FLAGS:
    -h, --help       Prints help information
    -V, --version    Prints version information

SUBCOMMANDS:
    help    Prints this message or the help of the given subcommand(s)
    sub     Subscribe to data on a stream
</code></pre>
<h4><a class="header" href="#hadron-pipeline-sub" id="hadron-pipeline-sub">hadron pipeline sub</a></h4>
<pre><code>Subscribe to data on a stream

USAGE:
    hadron pipeline sub &lt;pipeline&gt; &lt;stage&gt;

FLAGS:
    -h, --help       Prints help information
    -V, --version    Prints version information

ARGS:
    &lt;pipeline&gt;    The pipeline to which the subscription should be made
    &lt;stage&gt;       The pipeline stage to process
</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
